<!DOCTYPE html>
<html><head>
<meta charset='utf-8'>
<title>How Agentic AI is Changing the Landscape of Insider Threat Detection</title>
</head><body>
<h1>How Agentic AI is Changing the Landscape of Insider Threat Detection</h1>
<p><a href='https://medium.com/@laurencotton/how-agentic-ai-is-changing-the-landscape-of-insider-threat-detection-24ccd0141f7a?source=rss-01f840dd9ca6------2'>View on Medium</a></p>
<div><p><em>This is a review of the paper “Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation” for more information, view the original research </em><a href="https://arxiv.org/pdf/2508.07745"><strong><em>here</em></strong></a><strong><em>.</em></strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*w_4PbtLNI-_SgiYw9MwPLw.jpeg" /></figure><p>$17.4 million per organization in 2025 — that’s the <a href="https://deepstrike.io/blog/insider-threat-statistics-2025">cost</a> of insider threats to organizations— and there’s no sign of stopping.</p><p>Trusted access, genuine credentials, and a malicious user that is hiding in plain sight. An <a href="https://www.cisa.gov/topics/physical-security/insider-threat-mitigation/defining-insider-threats">insider threat</a> is the potential of an insider (a legitimately authorized user) to harm an organization. Because of the internal nature to insider threats, they are uniquely difficult to identify, and in turn difficult to prevent.</p><p>Machine learning models provide a new tool to identify insider threats before they become disastrous for an organization. By evaluating network and application logs, a machine learning model could spot patterns of malicious behaviors before it’s too late. However, an ML model is only as strong as the data it’s trained on, and good data is hard to come by. In order for a model to learn how to spot malicious insider behavior, it must understand what normal (benign) user activities look like. To get <em>real</em> data from a <em>real</em> organization is oftentimes infeasible due to the risks of publicizing trade secrets or leaking sensitive employee information.</p><blockquote>Researchers behind “Chimera” employed security experts to conduct realism tests on the industry standard synthetic data source (CERT). From this study, independent experts determined that current synthetic data was <a href="https://arxiv.org/pdf/2508.07745">significantly unrepresentative</a> of real-world activities.</blockquote><p>This presented a clear directive — how to create synthetic data that was diverse, realistic, and expansive.</p><p>With this in mind, Yu et al. chose to leverage Agentic AI to build a multi-agent enterprise simulator. But how does Chimera create this goldilocks dataset?</p><h4>Step 1: Create an Organization Profile</h4><p>Organizational profiling involves configuring the Chimera tool for a specific type of organization (technology, finance, or healthcare), setting an organization goal, as well as some basic structure and system deployment settings.</p><h4>Step 2: Add Employees and Operating Systems</h4><p>Here comes the magic — by setting specific operating systems (Windows/Mac) and applications (such as GitHub or Outlook) that are relevant to the organization’s goals, the data generated comes even closer to the goal of true realism. Employees are also added along with information about their personalities, names, credentials, and roles.</p><h4>Step 3: Threat Scheduling</h4><p>Once the environment and employees are set, Chimera begins planning realistic organizational activities for each employee agent. This includes scheduling tasks for the day, attending meetings with other agents, and executing independent work tasks that align with the organization’s goals. Ensuring that the tasks are realistic and use the same applications that a real employee would use is critical. Malicious agents are also embedded in the environment to execute their legitimate work assignments alongside threat directives.</p><p>The dataset gathered from the Chimera engine,<em> </em>ChimeraLog, was tested by independent experts and found to have an <a href="https://arxiv.org/pdf/2508.07745">average realism score of 4.20</a> on a 5-point Likert scale, indicating a close level of realism to real-world data. With this new and improved dataset, created using a multi-agent system, the team then tested many of the standard insider threat detection (ITD) models and found them to perform significantly worse when tested on ChimeraLog’s more realistic data. This indicates a false sense of security, and models that are likely to miss critical malicious activities.</p><p>Chimera (a configurable engine for simulating insider threats in an organization using multi-agent LLMs) is an important step forward in improving the accuracy of insider threat detection, as well an exciting new use case for multi-agent agentic AI. ChimeraLog is also a substantial contribution that will hopefully improve IDT models and set a new standard for how well organizations can identify insider threats.</p><p><a href="https://arxiv.org/pdf/2508.07745"><em>Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation</em></a><em> is still in the preprint stage at time of publishing and has not yet been peer-reviewed</em></p><p><em>This review was written without the use of AI writing tools except for grammar and spelling.</em></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=24ccd0141f7a" width="1" /></div>
</body></html>